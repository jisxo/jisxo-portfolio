# 04_oom_chunking

> 대외비 보호를 위해 식별 정보는 제거했으며, 구조와 원칙을 익명화하여 설명합니다.

## Claim
대용량 비정형 로그(배열 JSON) 처리 과정에서 발생하던 메모리 병목(OOM)을 **레코드 단위 Chunking**으로 완화하여 배치 처리를 안정화했습니다.

## Problem
배열(JSON array) 형태의 대용량 입력을 처리하는 배치 작업에서 메모리 사용량이 급증하여 OOM으로 실패하는 케이스가 발생했습니다.

## Root Cause (generalized) 
배열 전체를 한 번에 로딩/파싱하거나, 파싱 과정에서 중간 객체가 누적되면 메모리 사용량이 입력 크기에 비례해 증가할 수 있습니다.

## Fix — Record-level Chunking for JSON Array
- 배열 내부 레코드를 **N개 단위로 분할 처리**하여 한 번에 메모리에 올리는 데이터량을 제한했습니다.
- 각 chunk 처리 후 결과를 즉시 적재/flush하고, 다음 chunk로 진행하는 방식으로 누적 메모리 사용을 억제했습니다.
- 재실행/재처리를 고려해 처리 단위를 chunk 기준으로 명확히 분리했습니다(정책 수준).

### Pseudo flow
```text
# record-level chunking for large JSON array
for each chunk(records, size=N):
  process(chunk)
  upsert(chunk)
```
## Result
OOM으로 실패하던 배치 구간을 **분할 처리 기반으로 안정화**했으며, 재처리 가능한 실행 단위로 운영 리스크를 낮췄습니다.

## Trade-offs
- Chunk 크기가 커질수록 처리 효율은 좋아질 수 있으나, 메모리 위험이 증가합니다.
- Chunk 크기가 작아질수록 안정성은 좋아지나, 처리 오버헤드(반복 I/O)가 증가할 수 있습니다.
- 입력 형태(배열 JSON) 특성상 파싱/분할 방식에 따른 구현 복잡도가 존재합니다.

## Interview Q&A
- Q: 왜 레코드 단위로 나눴나요?  
  A: 한 번에 로딩/파싱되는 레코드 수를 제한해 메모리 사용량 상한을 만들기 위함입니다.
- Q: 재처리 시 중복 적재는 어떻게 방지하나요?  
  A: 테이블 유니크 키 기반의 upsert 등 멱등 정책을 전제로 처리 단위를 분리합니다(정책 수준).
- Q: 다른 대안은 없나요?  
  A: 입력을 JSONL로 변환하거나, 컬럼지향 포맷(Parquet) 전환 및 컬럼 프루닝으로 I/O와 메모리를 줄이는 접근도 가능합니다.