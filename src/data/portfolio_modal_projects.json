{
  "projects": [
    {
      "id": "security_platform",
      "title": "그룹사 통합 보안 데이터 플랫폼",
      "role": "데이터 엔지니어",
      "period": "2021.06 - 2022.09",
      "image": "/images/thumbnails/security_platform_phased.png",
      "domain": "보안 데이터 플랫폼",
      "summary": "이기종 보안 로그를 표준 스키마로 통합하고 JSON 청크 처리로 대용량 처리 병목을 완화한 프로젝트다.",
      "layout": "full",
      "keywords": [
        "데이터 표준화",
        "JSON 청크 처리",
        "Airflow",
        "ETL",
        "Python",
        "Docker"
      ],
      "overview": {
        "background": [
          "수작업 관제로 탐지 지연과 반복 운영 업무가 누적됐다.",
          "장비별 로그 포맷 차이로 통합 분석 기준을 맞추기 어려웠다.",
          "비정형 대용량 입력 구간에서 메모리 병목이 반복됐다."
        ],
        "objective": [
          "수집, 전처리, 탐지 흐름을 자동 실행 구조로 전환했다.",
          "대용량 JSON 입력을 청크 단위로 분할 처리하도록 경로를 구성했다.",
          "로그 타입별 필드를 표준 스키마로 매핑하는 규칙을 정의했다."
        ],
        "outcome": [
          "반복 수작업 구간을 파이프라인 실행으로 대체했다.",
          "청크 처리 적용으로 대용량 구간의 실패 빈도를 낮췄다.",
          "적재 전후 정합성 점검 기준을 운영 규칙으로 정리했다."
        ]
      },
      "design": [
        "로그 소스별 수집 어댑터를 분리해 입력 포맷 차이를 격리했다.",
        "표준 이벤트 스키마를 정의해 공통 키와 타입 체계를 명시했다.",
        "소스별 필드 매핑 규칙 테이블을 구성해 변환 로직을 고정했다.",
        "JSON 청크 크기 상한을 설정해 메모리 사용량을 통제했다.",
        "청크 실패를 분리 재시도하도록 실행 단위를 정의했다.",
        "적재 전 검증 단계에서 필수 키 누락 유입을 제한했다."
      ],
      "rules": [
        "키 정책: 이벤트 식별 키를 정의해 중복 판정과 재처리 기준으로 사용했다.",
        "정합성: 필수 항목 누락 레코드는 격리 영역으로 분기했다.",
        "재처리 단위: 청크 단위 재실행으로 전체 재적재를 피했다.",
        "표준화: 원천 필드는 매핑 규칙에 따라 표준 필드로만 노출했다."
      ],
      "quality_reprocess": [
        "수집 건수와 적재 건수 차이를 단계별로 점검했다.",
        "타입 캐스팅 실패 건을 별도 기록으로 분리했다.",
        "중복 키 충돌 건을 탐지해 반영 범위를 제한했다.",
        "장애 시 실패 청크만 재실행하는 절차를 유지했다."
      ],
      "limitations": [
        "대외비 대상 정보는 일반화해 기술했다.",
        "정량 성능 수치는 공개 가능한 근거 범위 내에서만 사용했다."
      ],
      "evidence": [
        {
          "title": "Canonical Schema v1 (Mail/Attachment)",
          "slug": "canonical-schema-v1"
        },
        {
          "title": "Parsing/Normalization Pipeline",
          "slug": "parsing-normalization-pipeline"
        },
        {
          "title": "Exception Handling Rules",
          "slug": "exception-handling-rules"
        },
        {
          "title": "OOM Mitigation (Record-level JSON Chunking)",
          "slug": "oom-mitigation-record-level-json-chunking"
        }
      ]
    },
    {
      "id": "telecom_pipeline",
      "title": "통신사 보안 로그 파이프라인 안정화",
      "role": "데이터 엔지니어",
      "period": "2025.04 - 2025.10",
      "image": "/images/thumbnails/telecom_log.png",
      "domain": "보안 운영 데이터 파이프라인",
      "summary": "DAG 선후행 규칙과 라벨 정합성 기준을 정리해 배치 실패 가능성을 낮춘 프로젝트다.",
      "layout": "full",
      "keywords": [
        "이메일 파싱",
        "라벨 기준 관리",
        "데이터 정합성",
        "Airflow",
        "ExternalTaskSensor",
        "Python"
      ],
      "overview": {
        "background": [
          "선행 작업 완료 전 후행 작업이 시작되어 배치 실패가 반복됐다.",
          "이메일 파싱 오류와 식별자 누락으로 데이터 일관성이 흔들렸다.",
          "라벨 기준이 분산돼 학습 데이터 축적 절차가 일관되지 않았다."
        ],
        "objective": [
          "DAG 의존성을 재정의해 선행 완료 후 후행 실행 조건을 명시했다.",
          "ExternalTaskSensor 조건을 조정해 스케줄 경합 유입을 제한했다.",
          "파싱 오류 보정과 식별자 누락 보정 로직을 분리 적용했다."
        ],
        "outcome": [
          "선후행 충돌로 인한 배치 실패 가능성을 낮췄다.",
          "파싱 점검 절차를 통해 로그 필드 누락을 줄였다.",
          "라벨 병합과 중복 제거 기준을 문서로 통일했다."
        ]
      },
      "design": [
        "수집 DAG와 후처리 DAG 의존성을 명시해 실행 경계를 분리했다.",
        "센서 타임아웃과 재시도 횟수를 분리해 실패 전파를 제어했다.",
        "파서 모듈과 식별자 보정 모듈을 분리해 책임 범위를 명확히 했다.",
        "라벨 병합 절차를 단계별 태스크로 분해해 추적성을 높였다.",
        "라벨 중복 제거 기준을 식별 키 기준으로 정의했다.",
        "기준 문서를 단일 문서로 통합해 운영 규칙을 일치시켰다."
      ],
      "rules": [
        "키 정책: 라벨 이벤트 식별 키를 정의해 중복 판정과 재처리 기준으로 사용했다.",
        "정합성: 파싱 필수 항목 누락 데이터는 학습 입력 영역 유입을 제한했다.",
        "재처리 단위: 태스크 단위 재실행을 기본 절차로 적용했다.",
        "의존성: 선행 태스크 성공 상태만 후행 실행 조건으로 허용했다."
      ],
      "quality_reprocess": [
        "선행 완료 시각과 후행 시작 시각을 비교 점검했다.",
        "파싱 오류 건을 유형별로 분리 기록해 보정 대상을 추적했다.",
        "라벨 병합 후 중복 키 건수를 점검해 규칙 준수 여부를 확인했다.",
        "장애 시 실패 태스크만 재실행해 영향 범위를 줄였다."
      ],
      "limitations": [
        "대외비 대상 정보는 일반화해 기술했다.",
        "성과 범위는 파이프라인 안정화와 입력 데이터 기준 정리에 한정했다."
      ]
    },
    {
      "id": "crypto_stream",
      "title": "코인스트림",
      "role": "데이터 엔지니어",
      "period": "개인 프로젝트",
      "image": "/images/thumbnails/crypto_streaming.png",
      "domain": "실시간 데이터 엔지니어링",
      "summary": "이벤트 시간과 워터마크, 멱등 처리 기준으로 실시간 수집·처리 구조를 검증한 개인 프로젝트다.",
      "layout": "half",
      "keywords": [
        "이벤트 시간",
        "워터마크",
        "멱등 처리",
        "Kafka",
        "Spark Streaming"
      ],
      "overview": {
        "background": [
          "배치 중심 처리 경험만으로는 실시간 처리 구조를 검증하기 어려웠다.",
          "지연 도착과 재실행 상황에서 순서 보존 기준이 필요했다."
        ],
        "objective": [
          "이벤트 시간 기준 집계와 워터마크 처리 범위를 정의했다.",
          "이벤트 식별 키 기반 멱등 반영 규칙을 적용했다."
        ],
        "outcome": [
          "수집, 처리, 저장의 기본 스트리밍 경로를 구성했다.",
          "지연 이벤트와 재실행 상황의 정합성 점검 절차를 마련했다."
        ]
      },
      "design": [
        "실시간 수집 경로와 결과 저장 경로를 분리해 처리 책임을 나눴다."
      ],
      "rules": [
        "키 정책: 거래 이벤트 식별 키를 정의해 중복 반영을 제한했다.",
        "워터마크: 허용 지연 범위를 넘긴 이벤트는 지연 처리 경로로 분기했다.",
        "멱등성: 동일 식별 키 이벤트는 단일 반영을 기준으로 처리했다.",
        "재처리: 시간 구간 단위 재실행으로 처리 범위를 제한했다."
      ],
      "quality_reprocess": [
        "입력 건수와 결과 건수 차이를 시간 구간 단위로 점검했다.",
        "재실행 시 중복 키 증가 여부를 점검해 멱등 규칙 준수 여부를 확인했다."
      ],
      "limitations": [
        "개인 실험 환경 기준 검증으로 운영 트래픽 성능 수치는 포함하지 않았다.",
        "외부 연계 장애에 대한 장기 운영 기록은 범위에서 제외했다."
      ]
    },
    {
      "id": "graph_reco_poc",
      "title": "공간-브랜드 그래프 기반 추천 PoC",
      "role": "데이터 엔지니어",
      "period": "2024.05 - 2024.11",
      "image": "/images/thumbnails/retail_gnn.png",
      "domain": "그래프 데이터 파이프라인",
      "summary": "단일 테이블을 그래프 입력으로 변환하고 분할 학습으로 메모리 제약 구간을 처리한 PoC 프로젝트다.",
      "layout": "half",
      "keywords": [
        "그래프 변환",
        "임베딩 학습",
        "메모리 분할",
        "GraphSAGE",
        "PoC"
      ],
      "overview": {
        "background": [
          "단일 테이블 기반 입력은 관계 신호를 직접 반영하기 어려웠다.",
          "그래프 학습을 위해 노드와 엣지 변환 단계가 필요했다."
        ],
        "objective": [
          "단일 테이블을 공간 노드와 브랜드 노드 구조로 변환했다.",
          "상호작용 데이터를 엣지로 변환해 임베딩 학습 입력을 구성했다."
        ],
        "outcome": [
          "그래프 변환부터 임베딩 저장까지 PoC 흐름을 검증했다.",
          "메모리 제약 구간에서 분할 학습 방식의 실행 가능성을 확인했다."
        ]
      },
      "design": [
        "전처리 단계에서 노드 속성 정규화 규칙을 정의했다."
      ],
      "rules": [
        "키 정책: 노드 식별 키와 엣지 식별 키를 분리 정의했다.",
        "정합성: 미존재 노드를 참조하는 엣지는 학습 입력에서 제외했다.",
        "재처리 단위: 분할 배치 단위 재학습으로 실패 구간만 재실행했다.",
        "메모리 제어: 분할 크기 상한을 설정해 단일 실행 메모리 사용량을 제한했다."
      ],
      "quality_reprocess": [
        "노드 수와 엣지 수의 전처리 전후 차이를 점검했다.",
        "실패한 분할 배치만 재실행해 전체 학습 반복을 줄였다."
      ],
      "limitations": [
        "PoC 범위로 작성했으며 온라인 추천 운영 결과는 포함하지 않았다.",
        "실제 서비스 트래픽 기반 성능 근거는 범위에서 제외했다."
      ]
    },
    {
      "id": "hidden_spot",
      "title": "Hidden Spot",
      "role": "데이터 엔지니어",
      "period": "개인 프로젝트",
      "image": "/images/thumbnails/hidden_spot_minio.png",
      "domain": "데이터 레이크",
      "summary": "원본 우선 저장과 ELT 분리 구조를 적용해 재처리 가능한 데이터 레이크 흐름을 구성한 개인 프로젝트다.",
      "layout": "full",
      "keywords": [
        "원본 우선 저장",
        "스키마 온 리드",
        "재처리",
        "MinIO",
        "ELT",
        "Python"
      ],
      "overview": {
        "background": [
          "리뷰 데이터 형식 변동이 잦아 고정 스키마 적재 비용이 높았다.",
          "원본 보존 체계가 약해 로직 변경 시 재처리 부담이 컸다.",
          "수집 저장 구조와 서비스 저장 구조가 결합돼 변경 영향 범위가 넓었다."
        ],
        "objective": [
          "원본 데이터를 객체저장소에 우선 적재하는 구조로 전환했다.",
          "정제 계층과 가공 계층을 원본 계층과 분리해 ELT 흐름을 구성했다.",
          "조회 시점 해석 방식으로 스키마 온 리드 적용 범위를 정의했다."
        ],
        "outcome": [
          "원본 재사용 기반 재처리 경로를 확보했다.",
          "서비스 계층이 파생 데이터만 참조하도록 분리해 결합도를 낮췄다.",
          "재처리 대상을 데이터셋 단위로 제한해 영향 범위를 줄였다."
        ]
      },
      "design": [
        "Raw 계층에 원천 데이터를 변경 없이 저장해 원본 보존 기준을 명시했다.",
        "정제 계층에 공통 정규화 규칙을 적용해 포맷 차이를 흡수했다.",
        "가공 계층에서 분석 목적 데이터셋을 분리 생성해 서빙과 분리했다.",
        "서빙 계층은 가공 결과만 조회하도록 접근 경로를 제한했다.",
        "데이터셋 메타 정보를 분리 저장해 재처리 대상 선택을 단순화했다.",
        "스키마 온 리드 해석 규칙을 문서화해 조회 로직 일관성을 유지했다."
      ],
      "rules": [
        "키 정책: 원천 데이터와 파생 데이터에 분리된 식별 키 체계를 적용했다.",
        "정합성: 정규화 실패 레코드는 격리 영역으로 분리해 서빙 경로 유입을 제한했다.",
        "재처리 단위: 데이터셋 파티션 단위 재처리를 기본으로 적용했다.",
        "계층 분리: Raw 계층 데이터는 변경 없이 보존하고 파생 계층에서만 변환했다.",
        "스키마 해석: 조회 시점 해석 규칙을 버전 단위로 관리했다."
      ],
      "quality_reprocess": [
        "원천 건수 대비 정제 성공 건수 차이를 배치 단위로 점검했다.",
        "정규화 실패 레코드를 유형별로 기록해 재처리 우선순위를 정했다.",
        "파생 데이터 재생성 시 대상 파티션만 실행해 전체 비용을 줄였다.",
        "파생 데이터 버전과 조회 결과 버전 일치 여부를 검증했다."
      ],
      "limitations": [
        "개인 프로젝트 범위로 운영 조직의 상시 모니터링 체계는 포함하지 않았다.",
        "외부 연동 구간의 실운영 성능 근거는 문서 범위에서 제외했다."
      ],
      "evidence": [
        {
          "title": "01_serving_api.md (Serving API Contract)",
          "slug": "hidden-spot-serving-api"
        },
        {
          "title": "02_async_jobs.md (Async Job Flow)",
          "slug": "hidden-spot-async-jobs"
        },
        {
          "title": "03_datalake_layout.md (Data Lake Layout)",
          "slug": "hidden-spot-datalake-layout"
        }
      ]
    }
  ]
}
